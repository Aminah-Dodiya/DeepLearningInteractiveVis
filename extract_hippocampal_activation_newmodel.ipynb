{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jkpM7-A4O0zs"
   },
   "outputs": [],
   "source": [
    "# Import data from Excel sheet\n",
    "import pandas as pd\n",
    "df = pd.read_excel('ADNI combined.xlsx', sheet_name='sample')\n",
    "#print(df)\n",
    "sid = df['RID']\n",
    "grp = df['Group at scan date (1=CN, 2=EMCI, 3=LMCI, 4=AD, 5=SMC)']\n",
    "age = df['Age at scan']\n",
    "sex = df['Sex (1=female)']\n",
    "tiv = df['TIV']\n",
    "field = df['MRI_Field_Strength']\n",
    "grpbin = (grp > 1) # 1=CN, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hqOOt-e9O8DY",
    "outputId": "bcc9b6d4-811c-465f-dc45-a73505a18f3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  663  nifti files\n"
     ]
    }
   ],
   "source": [
    "# Scan for nifti file names\n",
    "import glob\n",
    "dataAD = sorted(glob.glob('mwp1_MNI/AD/*.nii.gz'))\n",
    "dataLMCI = sorted(glob.glob('mwp1_MNI/LMCI/*.nii.gz'))\n",
    "dataCN = sorted(glob.glob('mwp1_MNI/CN/*.nii.gz'))\n",
    "dataFiles = dataAD + dataLMCI + dataCN\n",
    "numfiles = len(dataFiles)\n",
    "print('Found ', str(numfiles), ' nifti files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "aYME0L-yO-Bh",
    "outputId": "bb542980-c83b-4aee-834b-b8a93c4f6d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching covariates for loaded files ...\n",
      "Checking for scans not found in Excel sheet:  0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "debug = False\n",
    "cov_idx = [-1] * numfiles # list; array: np.full((numfiles, 1), -1, dtype=int)\n",
    "print('Matching covariates for loaded files ...')\n",
    "for i,id in enumerate(sid):\n",
    "    p = [j for j,x in enumerate(dataFiles) if re.search('_%04d_' % id, x)] # translate ID numbers to four-digit numbers, get both index and filename\n",
    "    if len(p)==0:\n",
    "        if debug: print('Did not find %04d' % id) # did not find Excel sheet subject ID in loaded file selection\n",
    "    else:\n",
    "        if debug: print('Found %04d in %s: %s' % (id, p[0], dataFiles[p[0]]))\n",
    "        cov_idx[p[0]] = i # store Excel index i for data file index p[0]\n",
    "print('Checking for scans not found in Excel sheet: ', sum(x<0 for x in cov_idx))\n",
    "\n",
    "labels = pd.DataFrame({'Group':grpbin}).iloc[cov_idx, :]\n",
    "grps = pd.DataFrame({'Group':grp, 'RID':sid}).iloc[cov_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWfYJA7FPEtQ"
   },
   "outputs": [],
   "source": [
    "#Load residualized data from disk\n",
    "import h5py\n",
    "import numpy as np\n",
    "hf = h5py.File('residuals_wb_mwp1_MNI.hdf5', 'r')\n",
    "hf.keys # read keys\n",
    "labels = np.array(hf.get('labels')) # note: was of data frame type before\n",
    "images = np.array(hf.get('images'))\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "colab_type": "code",
    "id": "BtCVM_QvPGjo",
    "outputId": "877ba485-d60d-40aa-fa0d-1a9dccfd3150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "# disable tensorflow deprecation warnings\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled=True\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1)\n",
    "    # device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4HGtE2qPwKH"
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "# define FOV to reduce required memory size\n",
    "x_range_from = 10; x_range_to = 110\n",
    "y_range_from = 10; y_range_to = 130\n",
    "z_range_from = 5; z_range_to = 105\n",
    "\n",
    "data_overlay = sorted(glob.glob('Hippocampus_masks/Hipp*_HarvOx_GMmasked0.5_MNI.nii*'))\n",
    "hippo_overlay = np.zeros((len(data_overlay), z_range_to-z_range_from, x_range_to-x_range_from, y_range_to-y_range_from, 1), dtype=np.float32) # numfiles× z × x × y ×1; avoid 64bit types\n",
    "\n",
    "for i in range(len(data_overlay)):   \n",
    "    img = nib.load(data_overlay[i])\n",
    "    img = img.get_data()[x_range_from:x_range_to, y_range_from:y_range_to, z_range_from:z_range_to]\n",
    "    img = np.transpose(img, (2, 0, 1)) # reorder dimensions to match coronal view z*x*y in MRIcron etc.\n",
    "    img = np.flip(img) # flip all positions\n",
    "    hippo_overlay[i, :,:,:, 0] = np.nan_to_num(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0gR3AIFmlIq-",
    "outputId": "a1194f5b-2ecf-42c2-d751-baefda29d81a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hippocampus_masks\\\\HippR_HarvOx_GMmasked0.5_MNI.nii'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### check which file corresponds to hippo l,r and both with indexes 0,1 and 2#####\n",
    "data_overlay[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNl6kUyZP08Q"
   },
   "outputs": [],
   "source": [
    "#hippos\n",
    "hippo_both = hippo_overlay[0,:,:,:,0]\n",
    "hippo_left = hippo_overlay[1,:,:,:,0]\n",
    "hippo_right = hippo_overlay[2,:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "52KlNjztPk-C",
    "outputId": "366cd49c-ccba-4eec-85a2-3dacbefa24ad",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load CNN model from disk\n",
    "from keras.models import load_model, Model\n",
    "#!pip install innvestigate\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy\n",
    "import csv\n",
    "\n",
    "\n",
    "# see https://github.com/albermax/innvestigate/blob/master/examples/notebooks/imagenet_compare_methods.ipynb for a list of alternative methods\n",
    "methods = [ # tuple with method,     params,                  label\n",
    "#            (\"deconvnet\",            {},                      \"Deconvnet\"),\n",
    "#            (\"guided_backprop\",      {},                      \"Guided Backprop\"),\n",
    "#            (\"deep_taylor.bounded\",  {\"low\": -1, \"high\": 1},  \"DeepTaylor\"),\n",
    "#            (\"input_t_gradient\",     {},                      \"Input * Gradient\"),\n",
    "#            (\"lrp.z\",                {},                      \"LRP-Z\"),\n",
    "#            (\"lrp.epsilon\",          {\"epsilon\": 1},          \"LRP-epsilon\"),\n",
    "            (\"lrp.alpha_1_beta_0\",   {\"neuron_selection_mode\":\"index\"},     \"LRP-alpha1beta0\"),\n",
    "]\n",
    "\n",
    "\n",
    "for k in range(20): #range(17,19):\n",
    "    mymodel = load_model('newmodel/newmodel_wb_cv%d.hdf5' % (k+1))\n",
    "    mymodel.name = 'newmodel_wb_cv%d_orig' % (k+1)\n",
    "    #mymodel.summary()\n",
    "    #model_wo_softmax = iutils.keras.graph.model_wo_softmax(mymodel)  ## sometimes raises: ValueError: The name \"dense_1\" is used 2 times in the model. All layer names should be unique.\n",
    "    #model_wo_softmax = Model(inputs=mymodel.inputs,\n",
    "    #                          outputs=iutils.keras.graph.pre_softmax_tensors(mymodel.outputs),\n",
    "    #                          name=('wo_softmax_cv%d' % (k+1))) \n",
    "    #\n",
    "    mymodel.layers[-1].activation=tf.keras.activations.linear\n",
    "    mymodel.save('tmp_wo_softmax.hdf5')\n",
    "    model_wo_softmax = load_model('tmp_wo_softmax.hdf5')\n",
    "    if (k==0):\n",
    "        model_wo_softmax.summary()\n",
    "\n",
    "    # create analyzer\n",
    "    analyzers = []\n",
    "    for method in methods:\n",
    "        #analyzer = innvestigate.create_analyzer(\"deep_taylor.bounded\", model_wo_softmax, **params )\n",
    "        analyzer = innvestigate.create_analyzer(method[0], model_wo_softmax, **method[1])\n",
    "        # Some analyzers require training.\n",
    "        #   analyzer.fit(test_img, batch_size=30, verbose=1)\n",
    "        #  analyzers.append(analyzer)\n",
    "\n",
    "    with open(('hipp_act_newmodel_cv%d.csv' % (k+1)), 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        csvwriter.writerow([\"subject_ID\", \"Sum_activation_of_right_hippocampal_volume\", \"Sum_activation_of_left_hippocampal_volume\", \"Sum_activation_of_both_hippocampal_volume\"])\n",
    "        if (k==7):\n",
    "            print('hipp_act_newmodel_cv%d.csv' % (k+1))\n",
    "            print(\"subject_ID Sum_activation_of_right_hippocampal_volume Sum_activation_of_left_hippocampal_volume Sum_activation_of_both_hippocampal_volume \")\n",
    "\n",
    "        for indx in range(len(grps)):\n",
    "            test_img = images[indx]\n",
    "            #test_orig = images_orig[indx]\n",
    "            #print('test image for subject of binary group: %d' % test_Y[subj_idx, 1]) # first col will indicate CN, second col indicates MCI/AD\n",
    "            #print('test image for subject of ADNI diagnosis: %d [1-CN, 3-LMCI, 4-AD]' % testgrps.Group.to_numpy(dtype=np.int)[subj_idx])\n",
    "\n",
    "            ####print('test subject ID %s' % grps.RID.to_numpy(dtype=np.int)[indx])\n",
    "\n",
    "            test_img = np.reshape(test_img, (1,)+ test_img.shape) # add first subj index again to mimic original array structure\n",
    "            #test_orig = np.reshape(test_orig, (1,)+ test_orig.shape) # add first subj index again to mimic original array structure\n",
    "\n",
    "            #for method,analyzer in zip(methods, analyzers):\n",
    "            a = np.reshape(analyzer.analyze(test_img, neuron_selection=1), test_img.shape[1:4])\n",
    "            np.clip(a, a_min=0, a_max=None, out=a)\n",
    "            a = scipy.ndimage.filters.gaussian_filter(a, sigma=0.8) # smooth activity image\n",
    "            scale = np.quantile(a, 0.99) # no need for abs(a)\n",
    "            if scale!=0:  # ignore if relevance maps contains only zeros, output will be zero as well\n",
    "                a = (a/scale)\n",
    "\n",
    "            overlay_act_both = hippo_both * a\n",
    "            overlay_act_l = hippo_left * a\n",
    "            overlay_act_r = hippo_right * a\n",
    "\n",
    "            csvwriter.writerow([grps.RID.to_numpy(dtype=np.int)[indx], np.sum(overlay_act_r), np.sum(overlay_act_l), np.sum(overlay_act_both)])\n",
    "            if (k==7):\n",
    "                print(grps.RID.to_numpy(dtype=np.int)[indx], np.sum(overlay_act_r), np.sum(overlay_act_l), np.sum(overlay_act_both))\n",
    "            #print('subject ID %s : Mean activation of left hippocampal volume %f : Sum activation of left hippocampal volume %f' % (grps.RID.to_numpy(dtype=np.int)[indx],np.mean(overlay_act_l[hippo_left>0]),np.sum(overlay_act_l[hippo_left>0])))\n",
    "            #print('subject ID %s : Mean activation of both hippocampal volume %f : Sum activation of both hippocampal volume %f' % (grps.RID.to_numpy(dtype=np.int)[indx],np.mean(overlay_act_both[hippo_both>0]),np.sum(overlay_act_both[hippo_both>0])))\n",
    "        csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "hippocamplal activity mean n sum.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
